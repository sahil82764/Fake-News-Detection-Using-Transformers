# Fake News Detection Using Transformers

This project uses a Transformer-based model (DistilBERT) to classify news articles as either real or fake. The pipeline includes scripts for data preprocessing, harmonization of multiple datasets, model training, and evaluation.

## Project Structure

```
├── data/                 # Data files (ignored by Git)
│   ├── raw/              # Raw datasets downloaded by the user
│   └── processed/        # Processed and split data generated by scripts
├── ml/                   # Core machine learning source code
│   ├── preprocess.py     # Harmonizes and cleans raw datasets
│   ├── datasets.py       # PyTorch Dataset class
│   └── train.py          # Model training script
├── scripts/              # Helper scripts
│   └── prepare_data.py   # Splits processed data into train/val/test sets
├── models/               # Saved model checkpoints (ignored by Git)
├── logs/                 # Log files (ignored by Git)
├── .gitignore            # Specifies files/directories for Git to ignore
├── requirements.txt      # Python dependencies
└── README.md             # This file
```

## Setup and Installation

Follow these steps to set up the project environment and download the necessary data.

### 1. Clone the Repository

```bash
git clone https://github.com/sahil82764/Fake-News-Detection-Using-Transformers.git
cd Fake-News-Detection-Using-Transformers
```

### 2. Create a Virtual Environment

It is highly recommended to use a virtual environment to manage project dependencies.

```bash
# Create the virtual environment
python -m venv .venv

# Activate it (on Windows)
.\.venv\Scripts\activate

# Activate it (on macOS/Linux)
# source .venv/bin/activate
```

### 3. Install Dependencies

Install all the required Python packages using the `requirements.txt` file.

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### 4. Download Raw Datasets

The preprocessing script requires two datasets. You must download them manually and place them in the `data/raw/` directory.

**First, create the necessary directories inside the `data/raw/` folder.**

**a. Kaggle Fake and Real News Dataset:**
*   **Download from:** https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset
*   Download the archive and extract `Fake.csv` and `True.csv`.
*   Place them inside `data/raw/kaggle_fake_news/`.

**b. LIAR Dataset:**
*   **Download from:** https://www.cs.ucsb.edu/~william/data/liar_dataset.zip
*   Download the zip file and extract `train.tsv`, `test.tsv`, and `valid.tsv`.
*   Place them inside `data/raw/liar/`.

After downloading and extracting, your `data/raw/` directory should look like this:

```
data/raw/
├── kaggle_fake_news/
│   ├── Fake.csv
│   └── True.csv
└── liar/
    ├── train.tsv
    ├── test.tsv
    └── valid.tsv
```

## How to Run the Pipeline

Run the scripts in the following order from the project's root directory.

### 1. Preprocess and Harmonize Data

This script loads the raw Kaggle and LIAR datasets, cleans them, combines them, and saves the result as a single Parquet file.

```bash
python ml/preprocess.py
```
This will generate `data/processed/processed_data.parquet`.

### 2. Split Data for Training

This script takes the processed data and splits it into training, validation, and test sets (80/10/10 split).

```bash
python scripts/prepare_data.py
```
This will generate `train.parquet`, `validation.parquet`, and `test.parquet` in the `data/processed/` directory.

### 3. Train the Model

This script fine-tunes the DistilBERT model on the preprocessed data. It will automatically use a GPU if one is available.

```bash
python ml/train.py
```
The best model checkpoint will be saved to `models/best_model.pt`. Logs for each step are saved in the `logs/` directory.